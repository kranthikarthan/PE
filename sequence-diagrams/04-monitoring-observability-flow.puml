@startuml MONITORING_OBSERVABILITY_FLOW
!theme plain
title Monitoring, Observability & Alerting Flow

participant "Client Request" as CR
participant "API Gateway" as AG
participant "Payment Processing Service" as MS
participant "Micrometer" as MM
participant "Prometheus" as P
participant "Jaeger" as J
participant "ELK Stack" as ELK
participant "Alert Manager" as AM
participant "Notification Service" as NS

== Request Tracing & Metrics Collection ==

CR -> AG: POST /api/v1/iso20022/comprehensive/pain001
note right: X-Correlation-ID: <correlation-id>\nX-Trace-ID: <trace-id>\nX-Span-ID: <span-id>

AG -> J: Start Trace Span
note right: Operation: api-gateway-request\nTrace ID: <trace-id>\nSpan ID: <span-id>\nTags: {service: api-gateway, endpoint: /api/v1/iso20022/comprehensive/pain001}

AG -> MM: Record Gateway Metrics
note right: Counter: api_gateway_requests_total\nLabels: {method: POST, endpoint: /api/v1/iso20022/comprehensive/pain001, status: processing}\nTimer: api_gateway_request_duration_seconds

AG -> AG: Process Request
AG -> MS: Forward Request
note right: Headers: X-Correlation-ID, X-Trace-ID, X-Span-ID

MS -> J: Create Child Span
note right: Operation: payment-processing\nParent Span ID: <span-id>\nSpan ID: <child-span-id>\nTags: {service: payment-processing, operation: pain001-processing}

MS -> MM: Record Payment Processing Metrics
note right: Counter: payment_processing_requests_total\nLabels: {message_type: pain001, tenant: <tenant-id>}\nTimer: payment_processing_duration_seconds

MS -> MS: Process ISO 20022 Message
MS -> MM: Record Business Metrics
note right: Counter: iso20022_messages_processed_total\nLabels: {message_type: pain001, tenant: <tenant-id>, success: true}\nGauge: active_connections\nHistogram: message_processing_time_seconds

MS -> ELK: Log Structured Event
note right: Log Level: INFO\nMessage: "Processing PAIN.001 message"\nFields: {correlation_id: <correlation-id>, trace_id: <trace-id>, tenant_id: <tenant-id>, message_type: pain001, processing_time_ms: <ms>}

MS --> AG: Return Response
AG -> J: Finish Trace Span
note right: Span ID: <span-id>\nDuration: <duration-ms>\nTags: {status: success, response_code: 200}

AG -> MM: Record Gateway Response Metrics
note right: Counter: api_gateway_requests_total\nLabels: {method: POST, endpoint: /api/v1/iso20022/comprehensive/pain001, status: 200}\nTimer: api_gateway_request_duration_seconds

AG --> CR: Return Response

== Metrics Collection & Export ==

MM -> P: Export Metrics
note right: Endpoint: /actuator/prometheus\nMetrics:\n- api_gateway_requests_total\n- payment_processing_requests_total\n- iso20022_messages_processed_total\n- circuit_breaker_state\n- rate_limiter_requests_total\n- webhook_delivery_attempts_total

P -> P: Store Metrics
note right: Storage: Time Series Database\nRetention: 30 days\nAggregation: 1m, 5m, 1h, 1d

== Distributed Tracing ==

J -> J: Store Trace Data
note right: Trace ID: <trace-id>\nSpans: [gateway-span, payment-processing-span, clearing-system-span]\nDuration: <total-duration>\nTags: {service: payment-engine, operation: pain001-to-pain002}

J -> J: Generate Trace Visualization
note right: Timeline: Request flow across services\nDependencies: Service call graph\nPerformance: Latency breakdown\nErrors: Exception tracking

== Log Aggregation & Analysis ==

ELK -> ELK: Process Log Events
note right: Elasticsearch: Index logs by timestamp\nLogstash: Parse and enrich log data\nKibana: Create dashboards and visualizations

ELK -> ELK: Generate Log Analytics
note right: Queries:\n- Error rate by service\n- Response time percentiles\n- User activity patterns\n- Security event analysis\n- Performance trends

== Alerting & Notification ==

P -> AM: Evaluate Alert Rules
note right: Rules:\n- High error rate (>5%)\n- Slow response time (>5s)\n- Circuit breaker open\n- Rate limit exceeded\n- Service down

alt Alert Condition Met
    AM -> AM: Generate Alert
    note right: Alert: High Error Rate Detected\nSeverity: WARNING\nService: payment-processing\nError Rate: 7.5%\nThreshold: 5%\nDuration: 5 minutes
    
    AM -> NS: Send Notification
    note right: Channels:\n- Email: ops-team@company.com\n- Slack: #alerts channel\n- SMS: +1234567890\n- PagerDuty: Critical alerts
    
    NS -> NS: Deliver Notifications
    note right: Email: Alert notification with details\nSlack: Formatted alert message\nSMS: Critical alert summary\nPagerDuty: Incident creation
    
else No Alert Condition
    AM -> AM: Continue Monitoring
    note right: Status: All metrics within thresholds\nNext Check: 30 seconds
end

== Health Checks & Service Discovery ==

AG -> AG: Perform Health Checks
note right: Services:\n- Payment Processing Service: /actuator/health\n- Core Banking: /actuator/health\n- Clearing System: /health\n- Redis: PING\n- Kafka: Metadata check

alt All Services Healthy
    AG -> MM: Record Health Metrics
    note right: Gauge: service_health_status\nLabels: {service: payment-processing, status: healthy}\nValue: 1
    
    AG -> ELK: Log Health Status
    note right: Log Level: INFO\nMessage: "All services healthy"\nServices: [payment-processing, core-banking, clearing-system, redis, kafka]
    
else Service Unhealthy
    AG -> MM: Record Health Metrics
    note right: Gauge: service_health_status\nLabels: {service: payment-processing, status: unhealthy}\nValue: 0
    
    AG -> ELK: Log Health Status
    note right: Log Level: ERROR\nMessage: "Service unhealthy"\nService: payment-processing\nError: Connection timeout
    
    AG -> AM: Trigger Health Alert
    note right: Alert: Service Unhealthy\nSeverity: CRITICAL\nService: payment-processing\nStatus: DOWN\nDuration: 2 minutes
end

== Performance Monitoring ==

MM -> MM: Collect Performance Metrics
note right: Metrics:\n- Request rate (req/sec)\n- Response time (P50, P95, P99)\n- Error rate (%)\n- Throughput (messages/sec)\n- Resource utilization (CPU, Memory)\n- Database connection pool\n- Cache hit ratio

MM -> P: Export Performance Metrics
note right: Endpoint: /actuator/prometheus\nPerformance Metrics:\n- http_server_requests_duration_seconds\n- jvm_memory_used_bytes\n- jvm_gc_pause_seconds\n- hikaricp_connections_active\n- redis_commands_duration_seconds

P -> P: Calculate Performance Trends
note right: Analysis:\n- Response time trends\n- Error rate patterns\n- Resource usage growth\n- Capacity planning data\n- SLA compliance metrics

== Error Tracking & Analysis ==

MS -> ELK: Log Error Event
note right: Log Level: ERROR\nMessage: "Payment processing failed"\nError: Insufficient funds\nStack Trace: <stack-trace>\nContext: {correlation_id: <correlation-id>, tenant_id: <tenant-id>, message_type: pain001}

ELK -> ELK: Analyze Error Patterns
note right: Analysis:\n- Error frequency by type\n- Error distribution by tenant\n- Error correlation with time\n- Error impact on SLA\n- Root cause analysis

ELK -> AM: Trigger Error Alert
note right: Alert: High Error Rate\nSeverity: WARNING\nError Type: Insufficient funds\nFrequency: 15 errors in 5 minutes\nThreshold: 10 errors in 5 minutes

== Capacity Planning & Scaling ==

P -> P: Analyze Resource Usage
note right: Metrics:\n- CPU utilization: 75%\n- Memory usage: 80%\n- Request rate: 850 req/sec\n- Response time P95: 2.5s\n- Error rate: 2%

P -> AM: Trigger Scaling Alert
note right: Alert: High Resource Usage\nSeverity: WARNING\nCPU: 75% (threshold: 70%)\nMemory: 80% (threshold: 75%)\nRecommendation: Scale up instances

AM -> NS: Send Scaling Notification
note right: Notification: "Consider scaling up services"\nDetails: High resource usage detected\nServices: payment-processing, api-gateway\nCurrent: 3 instances\nRecommended: 5 instances

== Security Monitoring ==

AG -> ELK: Log Security Event
note right: Log Level: WARN\nMessage: "Rate limit exceeded"\nEvent: SECURITY\nUser: <client-id>\nIP: <client-ip>\nLimit: 100 req/sec\nCurrent: 105 req/sec

ELK -> ELK: Analyze Security Patterns
note right: Analysis:\n- Failed authentication attempts\n- Rate limit violations\n- Suspicious IP addresses\n- Unusual access patterns\n- Security threat indicators

ELK -> AM: Trigger Security Alert
note right: Alert: Security Threat Detected\nSeverity: HIGH\nType: Rate limit violation\nUser: <client-id>\nIP: <client-ip>\nFrequency: 5 violations in 1 minute

AM -> NS: Send Security Notification
note right: Notification: "Security threat detected"\nChannels: [Email, Slack, SMS]\nRecipients: [Security Team, Operations Team]\nAction: Investigate immediately

== Monitoring Dashboard Updates ==

P -> P: Update Dashboard Metrics
note right: Dashboards:\n- System Overview\n- Service Health\n- Performance Metrics\n- Error Analysis\n- Security Monitoring\n- Business Metrics

ELK -> ELK: Update Log Dashboards
note right: Dashboards:\n- Log Analysis\n- Error Tracking\n- Security Events\n- User Activity\n- System Events

J -> J: Update Trace Dashboards
note right: Dashboards:\n- Service Dependencies\n- Request Flow\n- Performance Analysis\n- Error Tracing\n- Latency Breakdown

== Monitoring Features Summary ==

note over CR,NS
Monitoring & Observability Features:
- Distributed Tracing (Jaeger)
- Metrics Collection (Micrometer)
- Metrics Storage (Prometheus)
- Log Aggregation (ELK Stack)
- Health Checks & Service Discovery
- Performance Monitoring
- Error Tracking & Analysis
- Security Monitoring
- Capacity Planning
- Auto-scaling Recommendations
- Alert Management
- Multi-channel Notifications
- Dashboard Visualization
- SLA Monitoring
- Compliance Reporting
- Incident Response
- Root Cause Analysis
- Trend Analysis
- Anomaly Detection
- Real-time Monitoring
end note

@enduml